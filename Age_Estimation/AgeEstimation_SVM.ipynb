{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Estimation using SVM and LBP features\n",
    "In this project, we will train an SVM to predict the age of a person using LBP features computed on the face image. We will use \n",
    "\n",
    "## Data\n",
    "We will be using the IMDB Wiki dataset (https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/) for this project. We have already cleaned up the dataset to remove the non-face images as well as images with erroneous ages (age > 100 or age < 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as io\n",
    "import cv2\n",
    "import os\n",
    "from datetime import date, datetime\n",
    "from skimage import feature\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVR\n",
    "import glob\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import tqdm\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_image_date(mat_contents):\n",
    "    full_image_paths = mat_contents['imdb'][0][0][2][0]\n",
    "    dobs = mat_contents['imdb'][0][0][0][0]\n",
    "    years_taken = mat_contents['imdb'][0][0][1][0]\n",
    "\n",
    "    dict_data = {}\n",
    "    \n",
    "    for dobs, years_taken, img_path in zip(dobs, years_taken, full_image_paths):\n",
    "        img_path = img_path[0].split('/')[1]\n",
    "        dict_data[img_path] = years_taken - datetime.fromordinal(int(dobs)).year\n",
    "        \n",
    "    return dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_labels():\n",
    "    image_path = \"imdb_crop_resize\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    image_paths = glob.glob(os.path.join(image_path, '*.jp*g'))\n",
    "    random.shuffle(image_paths)\n",
    "    for image_path in tqdm.tqdm(image_paths):\n",
    "        # load the image, convert it to grayscale, and describe it\n",
    "        image = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        hist = describe(gray)\n",
    "\n",
    "        # extract the label from the image path, then update the label and data lists\n",
    "        labels.append(int(dict_data[os.path.split(image_path)[1]]))\n",
    "        data.append(hist)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBP Features\n",
    "Now, we will compute the Local Binary Pattern (LBP) features on a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def describe(image, eps=1e-7):\n",
    "        # compute the LBP representation of the image, and then use the LBP representation to build the histogram of patterns        \n",
    "        lbp_image = feature.local_binary_pattern(image, 24, 4, method=\"uniform\")\n",
    "        w, h = lbp_image.shape\n",
    "        \n",
    "        block_size = 32\n",
    "        num_blocks_w = int(w / block_size)\n",
    "        num_blocks_h = int(h / block_size)\n",
    "\n",
    "        hist_array = []\n",
    "        \n",
    "        # iterate over all blocks\n",
    "        for i in range(num_blocks_h):\n",
    "            for j in range(num_blocks_w):\n",
    "                # this is the current block in the image \n",
    "                curr_block = lbp_image[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
    "                \n",
    "                # get it's histogram\n",
    "                (hist, _) = np.histogram(curr_block, bins=np.arange(0, 17))\n",
    "                \n",
    "                # add it to the entire histogram of the image\n",
    "                hist_array.append(hist)\n",
    "        \n",
    "        \n",
    "        hist = np.array(hist_array)\n",
    "        \n",
    "        # make it flat (shape => [num_blocks_h*num_blocks_w*num_bins,])         \n",
    "        # After this shape would be (1024,)\n",
    "        hist = hist.ravel()\n",
    " \n",
    "        # normalize the histogram\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + eps)\n",
    " \n",
    "        # return the histogram of Local Binary Patterns\n",
    "        return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "Let's choose the best set of parameters for our SVM by running a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_parameters(X_train, y_train, X_test, y_test):\n",
    "    C_2d_range = [1e-2, 1, 1e2]\n",
    "    gamma_2d_range = [1e-1, 1, 1e1]\n",
    "    best_score = 0\n",
    "    best_C = 1 \n",
    "    best_gamma = 1\n",
    "    best_classifier = None\n",
    "    for C in C_2d_range:\n",
    "        for gamma in gamma_2d_range:\n",
    "            clf = svm.SVR(C=C, gamma=gamma).fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            if (score > best_score):\n",
    "                best_score = score\n",
    "                best_gamma = gamma\n",
    "                best_C = C\n",
    "                best_classifier = clf\n",
    "    return best_score, best_gamma, best_C, best_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_error(classifier, X_test, y_test):\n",
    "    prediction = classifier.predict(X_test)\n",
    "    error = mean_absolute_error(y_test, prediction)\n",
    "    return error        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca_transform(data):\n",
    "    pca = PCA(n_components=512)\n",
    "    pca.fit(data)\n",
    "    data_pca = pca.transform(data)\n",
    "    return data_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460723/460723 [5:06:40<00:00, 25.04it/s]   \n"
     ]
    }
   ],
   "source": [
    "mat_contents = io.loadmat(\"imdb.mat\")\n",
    "dict_data = retrieve_image_date(mat_contents)\n",
    "data, labels = get_data_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Before PCA =  10.005437032266032\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split (np.array(data), labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split (X_train, y_train, test_size=0.2, random_state=42)\n",
    "score, gamma, C, classifier = get_best_parameters(X_train, y_train, X_val, y_val)\n",
    "error = get_error(classifier, X_test, y_test)\n",
    "print(\"Error Before PCA = \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error After PCA =  10.00480897514508\n"
     ]
    }
   ],
   "source": [
    "pca_transform_data = pca_transform(np.array(data))\n",
    "X_train, X_test, y_train, y_test = train_test_split (pca_transform_data, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split (X_train, y_train, test_size=0.2, random_state=42)\n",
    "score, gamma, C, classifier = get_best_parameters(X_train, y_train, X_val, y_val)\n",
    "error = get_error(classifier, X_test, y_test)\n",
    "print(\"Error After PCA = \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "we can see that, even after reducing 50% of features by doing PCA, we are able to achieve the same average error while having a much lower dimensional support vector machine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
